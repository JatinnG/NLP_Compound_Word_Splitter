{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1c5cv2w2ImO"
   },
   "source": [
    "# Heuristic based compound word splitter for NLP applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjMLbnh61qG4"
   },
   "source": [
    "## A. Data Preparation\n",
    "\n",
    "In this project, we will use corpus word frequency data from:\n",
    "https://www.ids-mannheim.de/digspra/kl/projekte/methoden/derewo/. Refer to README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d_42aRJj-Q0W"
   },
   "outputs": [],
   "source": [
    "dereko_url = 'http://www.ids-mannheim.de/fileadmin/kl/derewo/DeReKo-2014-II-MainArchive-STT.100000.freq.zip'\n",
    "dereko_filename = 'DeReKo-2014-II-MainArchive-STT.100000.freq.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jZ2t0op754D",
    "outputId": "99d1a7d3-7935-41e0-a16a-bf61518265b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DeReKo-2014-II-MainArchive-STT.100000.freq.zip',\n",
       " <http.client.HTTPMessage at 0x266415c3310>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(dereko_url, dereko_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "35HmyuLu-IIW"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(dereko_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-HB0YdP-1nk",
    "outputId": "0cf41089-93da-483e-b384-49bcd12c1290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\t,\t$,\t500367688\n",
      ".\t.\t$.\t481370234\n",
      "der\tdie\tART\t241408360.16429\n",
      "die\tdie\tART\t188943867.569055\n",
      "und\tund\tKON\t1863515\n"
     ]
    }
   ],
   "source": [
    "print(open('DeReKo-2014-II-MainArchive-STT.100000.freq', encoding='utf-8').read()[:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNGSzbFG9vQC",
    "outputId": "803315a1-e509-4ba5-bb5c-73aa544c921d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on ['\\t', '$(', '156259594'], removing this entry.\n",
      "Loaded 99999 Words + POS + Frequencies.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Storing data in a Python list\n",
    "data = []\n",
    "with open('DeReKo-2014-II-MainArchive-STT.100000.freq', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Check data entries\n",
    "checked_data = []\n",
    "for entry in data:\n",
    "    try:\n",
    "        word, lemma, pos, freq = entry\n",
    "        checked_data.append(entry)\n",
    "    except ValueError:\n",
    "        # might happen with '\\t', we can ignore this one.\n",
    "        print('Error on %s, removing this entry.' % str(entry))\n",
    "data = checked_data\n",
    "\n",
    "print('Loaded %d Words + POS + Frequencies.' % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ilt_Y8sv_r0D",
    "outputId": "2f272d20-d6e9-4210-eaef-db4e8db77088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', ',', '$,', '500367688']\n",
      "['.', '.', '$.', '481370234']\n",
      "['der', 'die', 'ART', '241408360.16429']\n",
      "['in', 'in', 'APPR', '140040898']\n",
      "\n",
      "['Hohepriester', 'Hohepriester', 'NN', '3754.72208']\n",
      "['Graphic', 'UNKNOWN', 'NE', '3754.71645999999']\n",
      "['Klein-Winternheim', 'unknown', 'NE', '3754.5969999999']\n",
      "['Londoner', 'Londoner', 'NN', '3754.53195200004']\n"
     ]
    }
   ],
   "source": [
    "#data is in the sequence - words, stem, part-of-speech, frequency\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "print(data[2])\n",
    "print(data[5])\n",
    "print()\n",
    "print(data[-7])\n",
    "print(data[-6])\n",
    "print(data[-5])\n",
    "print(data[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NN': 41970, 'NE': 19699, 'ADJA': 12295, 'VVFIN': 6711, 'CARD': 4463, 'ADJD': 4455, 'VVPP': 3437, 'VVINF': 2792, 'ADV': 981, 'TRUNC': 545, 'VVIZU': 490, 'FM': 373, 'APPR': 220, 'PTKVZ': 176, 'PIAT': 152, 'PIS': 126, 'XY': 121, 'VMFIN': 89, 'VVIMP': 89, 'PROAV': 84, 'PPOSAT': 74, 'VAFIN': 72, 'KOUS': 66, 'PWAV': 50, 'APPRART': 49, 'PDS': 47, 'KON': 45, 'PDAT': 44, 'PPER': 39, 'ART': 35, 'ITJ': 35, 'PRELS': 18, 'PWS': 18, 'PAV': 15, '$(': 12, 'PTKANT': 12, 'APPO': 12, 'PRF': 11, 'KOUI': 9, 'PWAT': 9, '$.': 6, 'PTKA': 6, 'APZR': 6, 'VAIMP': 6, 'KOKOM': 5, 'VAINF': 5, 'VMINF': 5, 'VAPP': 4, 'PPOSS': 4, 'PTKNEG': 3, 'PTKZU': 3, 'VMPP': 3, 'PRELAT': 2, '$,': 1}\n",
      "99999\n"
     ]
    }
   ],
   "source": [
    "#Checking top occuring POS tags from the word frequency list to limit the number of POS components in compound splitter\n",
    "\n",
    "N_c = {}\n",
    "\n",
    "from collections import Counter\n",
    "for entry in data:\n",
    "        word, lemma, pos, freq = entry\n",
    "        if pos in N_c:\n",
    "            N_c[pos] += 1\n",
    "        else:\n",
    "            N_c[pos] = 1\n",
    "            \n",
    "N_c_sorted = dict(sorted(N_c.items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "print(N_c_sorted)\n",
    "print(sum(N_c_sorted.values()))\n",
    "#print(N_c.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P33L3zwSJ93"
   },
   "source": [
    "# B. Compound splitting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y7Pd_fyWSJTI"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CompoundSplitter(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def csplit(self, word):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xJcIFzEN0OwU"
   },
   "outputs": [],
   "source": [
    "## Some baseline splitters for comparison of results\n",
    "\n",
    "class BaselineSplitter1(CompoundSplitter):\n",
    "    # does not split anything\n",
    "\n",
    "    def csplit(self, word):\n",
    "        return word\n",
    "\n",
    "\n",
    "class BaselineSplitter2(CompoundSplitter):\n",
    "    # Splits if word starts with another word of 4-7 characters.\n",
    "    # Why 4-7? Too short words might not be useful here, too long words might be also compounds.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.components = []\n",
    "        self._init_components()\n",
    "\n",
    "    def _init_components(self):\n",
    "        # load word list from frequency data\n",
    "        for entry in data:\n",
    "            word, lemma, pos, freq = entry\n",
    "            if 4 <= len(word) <= 7:\n",
    "                self.components.append(word)\n",
    "\n",
    "    def csplit(self, word):\n",
    "        for component in self.components:\n",
    "            if word[:len(component)] == component:\n",
    "                return component + '|' + word[len(component):]\n",
    "        return word\n",
    "\n",
    "baseline_1 = BaselineSplitter1()\n",
    "baseline_2 = BaselineSplitter2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main splitter class which contains methods for generating word splits\n",
    "# As an exercise, you can also implement your own splitter here\n",
    "\n",
    "class MySplitter(CompoundSplitter):\n",
    "\n",
    "    import time \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.complete_list = []\n",
    "        self.components = []\n",
    "        self.frequencies = []\n",
    "        self.most_frequent = []\n",
    "        self.verbs = []\n",
    "        \n",
    "        #These are verb POS tags -- can be excluded from corpus\n",
    "        self.verbPOS = ['VAFIN','VAIMP','VAINF','VAPP','VMFIN','VMINF','VMPP','VVFIN','VVIMP','VVINF','VVIZU','VVPP']\n",
    "        \n",
    "        #These suffices can be present with lemmas to produce various inflected forms\n",
    "        self.JoinWords = ['s','es','n','er','e','en']\n",
    "        self._init_components()\n",
    "\n",
    "    def _init_components(self):\n",
    "        # load corresponding word list from frequency data\n",
    "        for entry in data:\n",
    "            word, lemma, pos, freq = entry\n",
    "            self.complete_list.append(word.encode('utf-8').lower())\n",
    "            self.frequencies.append([word.encode('utf-8').lower(), freq])\n",
    "            if float(freq) >= 4000000:\n",
    "                self.most_frequent.append(word.encode('utf-8').lower())\n",
    "            if pos in self.verbPOS:\n",
    "                self.verbs.append(word.encode('utf-8').lower())\n",
    "            if 4 <= len(word) <= 11:\n",
    "                self.components.append([word, lemma, freq])\n",
    "    \n",
    "    \n",
    "    def match_comp(self, word, component):\n",
    "        if word.encode('utf-8').lower() == component.encode('utf-8').lower():\n",
    "            return word\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    def list_diff(self, l1, l2):\n",
    "        diff =  list(set(l1) - set(l2)) + list(set(l2) - set(l1))\n",
    "        return str('|'.join(diff))\n",
    "        \n",
    "        \n",
    "    def split_heads(self, word):\n",
    "        '''Generates head splits for a given word -- first possible split reading word from left to right'''\n",
    "        head_splits = []\n",
    "        for entry in self.components:\n",
    "            comp, lemma, freq = entry\n",
    "            for end in (self.JoinWords + ['']):\n",
    "                component = comp+end\n",
    "                head = word[:len(component)]\n",
    "                match = self.match_comp(head, component) \n",
    "                if match:\n",
    "                    if match not in self.JoinWords:\n",
    "                        string = match + '|' + word[len(match):]\n",
    "                        if string not in head_splits:\n",
    "                            if string[-1] != '|':\n",
    "                                head_splits.append(string)\n",
    "        return head_splits\n",
    "   \n",
    "\n",
    "    def split_tails(self, word):\n",
    "        '''Generates tail splits for a given word -- first possible split reading word reverse from right to left'''\n",
    "        tail_splits = []\n",
    "        for entry in self.components:\n",
    "            component, lemma, freq = entry\n",
    "            tail = word[len(word) - len(component):]\n",
    "            match = self.match_comp(tail, component) \n",
    "            if match:\n",
    "                if match not in self.JoinWords:\n",
    "                    i = len(word)-len(match)\n",
    "                    if word[:i]:\n",
    "                        string = word[:i] + '|' + match\n",
    "                        if string not in tail_splits:\n",
    "                            tail_splits.append(string)\n",
    "        return tail_splits\n",
    "    \n",
    "    \n",
    "    def csplit(self, word):\n",
    "        '''Generates best possible split for a compound word -- main working function'''\n",
    "        import time\n",
    "        start = time.perf_counter()\n",
    "        sub_splits = []\n",
    "        self.base_splits = self.split_heads(word)\n",
    "        base_tails = self.split_tails(word)        \n",
    "        for split in base_tails:\n",
    "            if split not in self.base_splits:\n",
    "                self.base_splits.append(split)   \n",
    "        self.all_splits = self.base_splits\n",
    "        \n",
    "        for s in self.base_splits:\n",
    "            words = s.split('|')\n",
    "            for i in range(len(words)):\n",
    "                if len(words[i]) > 9:\n",
    "                    rem_words = words[:i] + words[i+1:]\n",
    "                    rem_heads = self.split_heads(words[i])\n",
    "                    rem_tails = self.split_tails(words[i])\n",
    "                    rem_splits = rem_heads + rem_tails\n",
    "                    for rem in rem_splits:\n",
    "                        if rem.split('|')[0] not in self.JoinWords:\n",
    "                            if i==0:\n",
    "                                string = rem + '|'.join(rem_words)\n",
    "                            if i==1:\n",
    "                                string = '|'.join(rem_words) + '|' + rem\n",
    "                            if string[-1] != '|' and string.replace('|','') == word:\n",
    "                                if string not in self.all_splits:\n",
    "                                    self.all_splits.append(string)       \n",
    "        \n",
    "        self.all_splits.append(word)\n",
    "        sp1 = time.perf_counter()\n",
    "        #print('Evaluation starts now. Time till now - in s: ', round(sp1 - start,3))\n",
    "    \n",
    "        self.evaluated_splits = dict()\n",
    "        for split in self.all_splits:\n",
    "            val = self.evaluate_split(split)\n",
    "            self.evaluated_splits[split] = val\n",
    "        self.sorted_evals =  dict(sorted(self.evaluated_splits.items(), key=lambda item: item[1], reverse = True)) \n",
    "        \n",
    "        sp2 = time.perf_counter()\n",
    "        #print('Evaluation time - in s: ', round(sp2 - sp1,3))\n",
    "        if self.sorted_evals:\n",
    "            return next(iter(self.sorted_evals))\n",
    "        else:\n",
    "            return word\n",
    "        \n",
    "        \n",
    "    def evaluate_split(self, split_to_eval):\n",
    "        '''Evaluate a split and assign score value to it'''\n",
    "        val = 1\n",
    "        words = split_to_eval.split('|')\n",
    "        for word in words:\n",
    "            freq = self.get_freq(word)\n",
    "            if len(word) <= 2:\n",
    "                val *= len(word)\n",
    "            elif len(word) == 3:\n",
    "                val *= 0.1*freq*len(word)\n",
    "            else:\n",
    "                val *= freq*len(word)\n",
    "        val = val**(1/len(words))\n",
    "        return [val, [len(word) for word in words]]\n",
    "    \n",
    "    \n",
    "    def get_freq(self, word):\n",
    "        '''Get frequency of closest lemma match'''\n",
    "        for entry in self.frequencies:\n",
    "            wd, freq = entry\n",
    "            if wd == word.encode('utf-8').lower():\n",
    "                    return float(freq)\n",
    "            else:\n",
    "                for end in self.JoinWords:\n",
    "                    end = end.encode('utf-8')\n",
    "                    if (wd+end) == word.encode('utf-8').lower():\n",
    "                        return float(freq)\n",
    "                    if (end+wd) == word.encode('utf-8').lower():\n",
    "                        return float(freq)\n",
    "        return 0.5\n",
    "    \n",
    "    \n",
    "    def csplit_head(self, word):\n",
    "        '''Used only if head side splits are required'''\n",
    "        pred = self.csplit(word)\n",
    "        for split in self.all_splits:\n",
    "            if split.count('|') > 1:\n",
    "                del self.sorted_evals[split]\n",
    "        if self.sorted_evals:\n",
    "            return next(iter(self.sorted_evals))\n",
    "        else:\n",
    "            return word\n",
    "          \n",
    "    \n",
    "    def csplit_test(self, word):\n",
    "        '''Can be used for debugging'''\n",
    "        pred = self.csplit(word)\n",
    "        #print(self.base_splits)\n",
    "        print(self.sorted_evals)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def csplit_test_head(self, word):\n",
    "        pred = self.csplit_head(word)\n",
    "        print(self.sorted_evals)\n",
    "        return pred\n",
    "    \n",
    "my_splitter = MySplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stammes|organisation\n"
     ]
    }
   ],
   "source": [
    "print(my_splitter.csplit('Stammesorganisation'))\n",
    "#print(my_splitter.csplit('Ausbildungsende'))\n",
    "#print(my_splitter.csplit('Oberstufenunterricht'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telephon|gespräch\n",
      "Wasser|schüssel\n"
     ]
    }
   ],
   "source": [
    "print(my_splitter.csplit('Telephongespräch'))\n",
    "print(my_splitter.csplit('Wasserschüssel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuttgarter\n",
      "Bus|fahrerin\n"
     ]
    }
   ],
   "source": [
    "print(my_splitter.csplit('Stuttgarter'))\n",
    "print(my_splitter.csplit('Busfahrerin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0waEPv__STIE"
   },
   "source": [
    "# C. Evaluating results quality\n",
    "\n",
    "Download the file `csplits_train.tsv` and `csplits_test.tsv`.\n",
    "\n",
    "This data set was created from a mixture of German social media data from Twitter and Reddit. This data set is created automatically, there might be errors (tokenization/POS/compound splits). The words were tokenized, tagged and lemmatized using TreeTagger.\n",
    "\n",
    "While `csplits_train.tsv` contains lemmata (extracted from the TreeTagger output) with the predicted splits of a compound splitter (we used CompoST (Cap, 2014) based on SMOR; you should not use these tools, you should develop your own!), `NLP2021_csplits_test.tsv` only contains lemmata. These lists contain **only nouns** (NN according to TreeTagger) and each word has at least 10 characters ([A-Za-zÄÜÖöüäß], and only those are allowed - so no numbers, etc.).\n",
    "\n",
    "The TreeTagger is a tool for annotating text with part-of-speech and lemma information. It was developed by Helmut Schmid in the TC project at the Institute for Computational Linguistics of the University of Stuttgart. Refer https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/ for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaZsJwo7cr2a",
    "outputId": "c2361b30-4f07-4b98-ef8d-dcb1596e217b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 20000 training data instances.\n",
      "Read 8345 test data instances.\n"
     ]
    }
   ],
   "source": [
    "train_data_file = 'csplits_train.tsv'\n",
    "train_data_instances = [instance.split('\\t') for instance in\n",
    "                        open(train_data_file).read().split('\\n')\n",
    "                        if instance.strip()]\n",
    "print('Read %d training data instances.' % len(train_data_instances))\n",
    "\n",
    "test_data_file = 'csplits_test.tsv'\n",
    "test_data_instances = open(test_data_file).read().split('\\n')[:-1]\n",
    "print('Read %d test data instances.' % len(test_data_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvIYLJd4ucSC",
    "outputId": "d53d3bba-9905-4878-d269-e63360b1aa68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training data with predicted splits:\n",
      "['Grundstandard', 'Grund|standard']\n",
      "['Stammesorganisation', 'Stammes|organisation']\n",
      "['Heilerziehungspflegerin', 'Heil|erziehungs|pflegerin']\n",
      "['Verbesserung', 'Verbesserung']\n",
      "\n",
      "** Test data:\n",
      "Beschleunigungsrennen\n",
      "Paketinhalt\n",
      "Lagerverwaltung\n"
     ]
    }
   ],
   "source": [
    "print('** Training data with predicted splits:')\n",
    "print(train_data_instances[1])\n",
    "print(train_data_instances[2])\n",
    "print(train_data_instances[0])\n",
    "print(train_data_instances[51])\n",
    "\n",
    "print('\\n** Test data:')\n",
    "print(test_data_instances[0])\n",
    "print(test_data_instances[10])\n",
    "print(test_data_instances[250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4MBwvW8uOgn"
   },
   "source": [
    "Splits are marked with vertical bars ('|'). Note that some words are split multiple times (>1 bars) while some words are not split at all (no bars) - some words here are simply long nouns (e.g. derived nouns) and not compounds. Also note that the components of these words can be combined with joint elements (\"Fugenelemente\").\n",
    "For example, `Verkehrsinformationssystem` is a compound of `Verkehr`, `Information` and `System` with two additional `s` inserted: `Verkehr**s**|information**s**|system`.\n",
    "\n",
    "Also note that verb components often occur in stem form: \n",
    "`Denkvermögen` is a compound of the verb `denken` and the noun `vermögen`; the stem of `denken` is `denk`, thus, we get `Denk|vermögen`.\n",
    "\n",
    "Joint elements can also occur with verb stems:\n",
    "`Funkmeldesystem` is a compound of `Funk`, `melden` and `System`. `meld` is the stem of of `melden`. Thus the additional `e` is a joint element in the split form `Funk|meld**e**|system` .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RXqfQmQbuAp"
   },
   "source": [
    "## C.1 Evaluation on train data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "eAS5ynkcYiFz"
   },
   "outputs": [],
   "source": [
    "#Defining some function that calls the above splitter methods and compare with gold standard labels\n",
    "\n",
    "def evaluate(compound_splitter, labeled_instances):\n",
    "    tp = 0\n",
    "    for labeled_instance in labeled_instances:\n",
    "        word, split = labeled_instance\n",
    "        pred = compound_splitter.csplit(word)\n",
    "        if pred == split:\n",
    "            tp += 1\n",
    "    acc = tp/len(labeled_instances)\n",
    "    return acc\n",
    "\n",
    "\n",
    "#Smaller version of evaluate method to run preliminary tests on a part of the trained dataset - first n entries\n",
    "def evaluate_2(compound_splitter, labeled_instances, n):\n",
    "    tp = 0\n",
    "    for labeled_instance in labeled_instances[:n]:\n",
    "        word, split = labeled_instance\n",
    "        pred = compound_splitter.csplit(word)\n",
    "        if pred == split:\n",
    "            tp += 1\n",
    "    acc = tp/len(labeled_instances[:n])\n",
    "    return acc\n",
    "    \n",
    "    \n",
    "#Function to evaluate first n entries and save results in a file to compare results\n",
    "def evaluate_and_save(compound_splitter, labeled_instances, n, filename):\n",
    "    with open(filename, mode='w') as outfile:\n",
    "        outfile.truncate()\n",
    "        tp = 0\n",
    "        i = 0\n",
    "        for labeled_instance in labeled_instances[:n]:\n",
    "            i += 1\n",
    "            if i%(n/10) == 0:\n",
    "                print('Completed ', i, 'instances')\n",
    "            word, split = labeled_instance\n",
    "            pred = compound_splitter.csplit(word)\n",
    "            if pred == split:\n",
    "                tp += 1\n",
    "                outfile.write('Correct -  ' + word + '\\t\\t Predicted:' + pred + '\\t\\t Correct:' + split + '\\n')\n",
    "            else:\n",
    "                outfile.write('Incorrect -  ' + word + '\\t\\t Predicted:' + pred + '\\t\\t Correct:' + split + '\\n')\n",
    "    outfile.close()\n",
    "    acc = tp/len(labeled_instances[:n])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPlej8g2X9v7",
    "outputId": "2e1c6512-90dc-482c-980d-51b0d0fd8c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of baseline_1: 12.00%\n",
      "Accuracy of baseline_2: 28.00%\n"
     ]
    }
   ],
   "source": [
    "baseline_1 = BaselineSplitter1()\n",
    "print('Accuracy of baseline_1: %2.2f%%' % (100*evaluate_2(baseline_1, train_data_instances, 100)))\n",
    "\n",
    "baseline_2 = BaselineSplitter2()\n",
    "print('Accuracy of baseline_2: %2.2f%%' % (100*evaluate_2(baseline_2, train_data_instances, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  5 instances\n",
      "Completed  10 instances\n",
      "Completed  15 instances\n",
      "Completed  20 instances\n",
      "Completed  25 instances\n",
      "Completed  30 instances\n",
      "Completed  35 instances\n",
      "Completed  40 instances\n",
      "Completed  45 instances\n",
      "Completed  50 instances\n",
      "Accuracy of my splitter - on first n instances: 72.00%\n",
      "Time taken to evaluate and save n instances in secs is:  220.03\n"
     ]
    }
   ],
   "source": [
    "#Evaluate first n instances and save results in txt file for comparison\n",
    "import time\n",
    "filename = 'Test_Eval_first50.txt'\n",
    "n = 100\n",
    "\n",
    "start = time.perf_counter()\n",
    "print('Accuracy of my splitter - on first n instances: %2.2f%%' % (100*evaluate_and_save(my_splitter, train_data_instances, n, filename)))\n",
    "end = time.perf_counter()\n",
    "print('Time taken to evaluate and save n instances in secs is: ', round(end-start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bank|aktivität': [235250.6766834051, [4, 9]], 'Bankaktivität': [0.5, [13]]}\n",
      "Bank|aktivität\n"
     ]
    }
   ],
   "source": [
    "#Bankaktivität\n",
    "word = 'Bankaktivität'\n",
    "print(my_splitter.csplit_test(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkrWoLWkb3G9"
   },
   "source": [
    "## C.2 Prediction of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8qzgcBvCcO1w"
   },
   "outputs": [],
   "source": [
    "def predict_and_save(compound_splitter, instances, filename):\n",
    "    with open(filename, mode='w') as outfile:\n",
    "        for word in instances:\n",
    "            pred = compound_splitter.csplit(word)\n",
    "            outfile.write(word + '\\t' + pred + '\\n')\n",
    "\n",
    "            \n",
    "#Final evaluation of all words and saving data in a txt file\n",
    "def predict_and_save_final(compound_splitter, instances, filename):\n",
    "    with open(filename, mode='w') as outfile:\n",
    "        outfile.truncate()\n",
    "        i = 0\n",
    "        for word in instances[:]:\n",
    "            i += 1\n",
    "            if i%500 == 0:\n",
    "                print('Completed ', i, 'instances')\n",
    "            pred = compound_splitter.csplit(word)\n",
    "            outfile.write(word + '\\t' + pred + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "id": "MbJ0aBiWYIF_"
   },
   "outputs": [],
   "source": [
    "# Predicting test data with simple baselines\n",
    "baseline_1 = BaselineSplitter1()\n",
    "predict_and_save(baseline_1, test_data_instances, 'pred-components_baseline_1.csv')\n",
    "\n",
    "baseline_2 = BaselineSplitter2()\n",
    "predict_and_save(baseline_2, test_data_instances, 'pred-components_baseline_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TJd6tFRndmI5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  500 instances\n",
      "Completed  1000 instances\n",
      "Completed  1500 instances\n",
      "Completed  2000 instances\n",
      "Completed  2500 instances\n",
      "Completed  3000 instances\n",
      "Completed  3500 instances\n",
      "Completed  4000 instances\n"
     ]
    }
   ],
   "source": [
    "# Predicting test data using our own splitter here -- saving results in csv file\n",
    "filename = 'pred-components.csv'\n",
    "\n",
    "my_splitter = MySplitter()\n",
    "predict_and_save_final(my_splitter, test_data_instances, filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-2021_exercise4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
